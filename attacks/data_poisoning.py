"""
æ•°æ®æ±¡æŸ“æ”»å‡»æ¨¡å—
å®ç°å„ç§æ•°æ®æ±¡æŸ“æ”»å‡»æ–¹æ³•
"""

import torch
import random
import numpy as np
from abc import ABC, abstractmethod


class DataPoisoningBase(ABC):
    """æ•°æ®æ±¡æŸ“æ”»å‡»åŸºç±»"""
    
    def __init__(self, poison_rate=0.5, num_classes=10):
        """
        Args:
            poison_rate: æ±¡æŸ“æ¯”ä¾‹ (0.0-1.0)
            num_classes: ç±»åˆ«æ•°é‡
        """
        self.poison_rate = poison_rate
        self.num_classes = num_classes
    
    @abstractmethod
    def poison_data(self, images, labels):
        """
        æ±¡æŸ“æ•°æ®çš„æŠ½è±¡æ–¹æ³•
        Args:
            images: è¾“å…¥å›¾åƒ
            labels: åŸå§‹æ ‡ç­¾
        Returns:
            poisoned_images, poisoned_labels: æ±¡æŸ“åçš„å›¾åƒå’Œæ ‡ç­¾
        """
        pass
    
    def should_poison(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œæ±¡æŸ“"""
        return random.random() < self.poison_rate


class LabelFlippingAttack(DataPoisoningBase):
    """æ ‡ç­¾ç¿»è½¬æ”»å‡»"""
    
    def __init__(self, poison_rate=0.5, num_classes=10, flip_strategy='random'):
        """
        Args:
            poison_rate: æ±¡æŸ“æ¯”ä¾‹
            num_classes: ç±»åˆ«æ•°é‡
            flip_strategy: ç¿»è½¬ç­–ç•¥ ('random', 'targeted', 'next_class')
        """
        super().__init__(poison_rate, num_classes)
        self.flip_strategy = flip_strategy
        self.target_class = 0  # ç›®æ ‡æ”»å‡»ç±»åˆ«ï¼ˆç”¨äºtargetedç­–ç•¥ï¼‰
    
    def poison_data(self, images, labels):
        """
        æ‰§è¡Œæ ‡ç­¾ç¿»è½¬æ”»å‡»
        """
        poisoned_labels = labels.clone()
        
        for i in range(len(labels)):
            if self.should_poison():
                original_label = labels[i].item()
                poisoned_labels[i] = self._flip_label(original_label)
        
        return images, poisoned_labels  # å›¾åƒä¸å˜ï¼Œåªæ”¹æ ‡ç­¾
    
    def _flip_label(self, original_label):
        """æ ¹æ®ç­–ç•¥ç¿»è½¬æ ‡ç­¾"""
        if self.flip_strategy == 'random':
            # éšæœºç¿»è½¬åˆ°å…¶ä»–ç±»åˆ«
            possible_labels = [j for j in range(self.num_classes) if j != original_label]
            return random.choice(possible_labels) if possible_labels else original_label
        
        elif self.flip_strategy == 'targeted':
            # ç¿»è½¬åˆ°æŒ‡å®šç›®æ ‡ç±»åˆ« (æ›´æ¿€è¿›)
            return self.target_class
        
        elif self.flip_strategy == 'next_class':
            # ç¿»è½¬åˆ°ä¸‹ä¸€ä¸ªç±»åˆ«ï¼ˆå¾ªç¯ï¼‰
            return (original_label + 1) % self.num_classes
        
        else:
            raise ValueError(f"Unknown flip strategy: {self.flip_strategy}")


class NoiseInjectionAttack(DataPoisoningBase):
    """å™ªå£°æ³¨å…¥æ”»å‡»"""
    
    def __init__(self, poison_rate=0.3, noise_std=0.1):
        """
        Args:
            poison_rate: æ±¡æŸ“æ¯”ä¾‹
            noise_std: å™ªå£°æ ‡å‡†å·®
        """
        super().__init__(poison_rate)
        self.noise_std = noise_std
    
    def poison_data(self, images, labels):
        """
        å‘å›¾åƒæ³¨å…¥é«˜æ–¯å™ªå£°
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ‰¹é‡ç”Ÿæˆå™ªå£°ï¼Œåœ¨CPUä¸Šç”Ÿæˆä»¥å‡å°‘GPUå‹åŠ›
        """
        poisoned_images = images.clone()
        
        # ğŸ”§ ä¼˜åŒ–1: ä¸€æ¬¡æ€§ç¡®å®šæ‰€æœ‰éœ€è¦æ±¡æŸ“çš„æ ·æœ¬ï¼ˆæ‰¹é‡maskï¼‰
        batch_size = len(images)
        mask = torch.rand(batch_size) < self.poison_rate
        num_poison = mask.sum().item()
        
        if num_poison > 0:
            # ğŸ”§ ä¼˜åŒ–2: åœ¨CPUä¸Šæ‰¹é‡ç”Ÿæˆæ‰€æœ‰å™ªå£°ï¼Œå‡å°‘GPUéšæœºæ•°è°ƒç”¨
            noise = torch.randn(num_poison, *images.shape[1:]) * self.noise_std
            
            # ğŸ”§ ä¼˜åŒ–3: ä¸€æ¬¡æ€§è½¬ç§»åˆ°GPUå¹¶åº”ç”¨
            noise = noise.to(images.device)
            poisoned_images[mask] = torch.clamp(
                images[mask] + noise, 0, 1
            )
        
        # ğŸ”§ ä¼˜åŒ–4: å¼ºåˆ¶GPUåŒæ­¥ï¼Œé¿å…å¼‚æ­¥æ“ä½œç´¯ç§¯
        if images.is_cuda:
            torch.cuda.synchronize()
        
        return poisoned_images, labels


class BackdoorAttack(DataPoisoningBase):
    """åé—¨æ”»å‡»"""
    
    def __init__(self, poison_rate=0.1, trigger_size=3, target_class=0):
        """
        Args:
            poison_rate: æ±¡æŸ“æ¯”ä¾‹
            trigger_size: è§¦å‘å™¨å¤§å°
            target_class: ç›®æ ‡ç±»åˆ«
        """
        super().__init__(poison_rate)
        self.trigger_size = trigger_size
        self.target_class = target_class
    
    def poison_data(self, images, labels):
        """
        åœ¨å›¾åƒä¸Šæ·»åŠ è§¦å‘å™¨å¹¶ä¿®æ”¹æ ‡ç­¾
        """
        poisoned_images = images.clone()
        poisoned_labels = labels.clone()
        
        for i in range(len(images)):
            if self.should_poison():
                # åœ¨å³ä¸‹è§’æ·»åŠ ç™½è‰²æ–¹å—ä½œä¸ºè§¦å‘å™¨
                poisoned_images[i, :, -self.trigger_size:, -self.trigger_size:] = 1.0
                poisoned_labels[i] = self.target_class
        
        return poisoned_images, poisoned_labels


class MixedAttack(DataPoisoningBase):
    """æ··åˆæ”»å‡»ï¼šç»“åˆå¤šç§æ”»å‡»æ–¹æ³•"""
    
    def __init__(self, attacks_config):
        """
        Args:
            attacks_config: æ”»å‡»é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (attack_instance, weight)
        """
        self.attacks = attacks_config
        total_weight = sum(weight for _, weight in attacks_config)
        self.weights = [weight / total_weight for _, weight in attacks_config]
    
    def poison_data(self, images, labels):
        """
        éšæœºé€‰æ‹©ä¸€ç§æ”»å‡»æ–¹æ³•æ‰§è¡Œ
        """
        attack_idx = np.random.choice(len(self.attacks), p=self.weights)
        selected_attack, _ = self.attacks[attack_idx]
        return selected_attack.poison_data(images, labels)
