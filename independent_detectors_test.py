#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹æ£€æµ‹å™¨æµ‹è¯•ç³»ç»Ÿ
æµ‹è¯•8ä¸ªæ£€æµ‹å™¨çš„ç‹¬ç«‹æ£€æµ‹æ•ˆæœï¼Œä¸è¿›è¡Œèšåˆ

æ£€æµ‹å™¨åˆ—è¡¨ï¼š
1. Euclidean Distanceï¼ˆæ¬§æ°è·ç¦»ï¼‰
2. Parameter Statisticsï¼ˆå‚æ•°ç»Ÿè®¡ï¼‰
3. Cosine Similarityï¼ˆå‚æ•°ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
4. Parameter Update Normï¼ˆå‚æ•°æ›´æ–°èŒƒæ•°ï¼‰- æ–°å®ç°
5. Update Directionï¼ˆæ›´æ–°æ–¹å‘ï¼‰- æ–°å®ç°
6. Layer-wise Update Normï¼ˆå±‚çº§æ›´æ–°èŒƒæ•°ï¼‰- æ–°å®ç°
7. Layer Coordinationï¼ˆå±‚åè°ƒï¼‰- å·²ç¦ç”¨ä½†æµ‹è¯•
8. Gradient Normï¼ˆæ¢¯åº¦èŒƒæ•°ï¼‰- æ³¨é‡Šæ‰
"""

import torch
import torch.nn as nn
import numpy as np
import copy
import json
from datetime import datetime


# ===================== æ–°å®ç°çš„æ£€æµ‹å™¨ =====================

class ParameterUpdateNormDetector:
    """å‚æ•°æ›´æ–°èŒƒæ•°æ£€æµ‹å™¨ - ä¸éœ€è¦é¢å¤–è®­ç»ƒ"""
    
    def __init__(self, args):
        self.args = args
        self.device = args.device
    
    def calculate_update_norm(self, global_model, external_model):
        """è®¡ç®—å‚æ•°æ›´æ–°èŒƒæ•°ï¼ˆä¼°ç®—æ¢¯åº¦èŒƒæ•°ï¼‰"""
        global_params = dict(global_model.state_dict())
        external_params = dict(external_model.state_dict())
        
        layer_norms = []
        layer_details = {}
        
        for layer_name in global_params.keys():
            if layer_name in external_params:
                # è®¡ç®—å‚æ•°æ›´æ–°
                param_update = external_params[layer_name] - global_params[layer_name]
                
                # ä¼°ç®—æ¢¯åº¦ (SGD: grad = -update / lr)
                estimated_gradient = -param_update / self.args.lr
                
                # è®¡ç®—èŒƒæ•°
                layer_norm = torch.norm(estimated_gradient).item()
                layer_norms.append(layer_norm)
                layer_details[layer_name] = layer_norm
        
        # ç»Ÿè®¡ç‰¹å¾
        total_norm = np.sqrt(sum(n**2 for n in layer_norms))
        mean_norm = np.mean(layer_norms)
        std_norm = np.std(layer_norms)
        max_norm = np.max(layer_norms)
        min_norm = np.min(layer_norms)
        cv_norm = std_norm / mean_norm if mean_norm > 0 else 0
        
        return {
            'total_norm': total_norm,
            'mean_norm': mean_norm,
            'std_norm': std_norm,
            'max_norm': max_norm,
            'min_norm': min_norm,
            'cv_norm': cv_norm,
            'layer_norms': layer_details
        }
    
    def detect_anomaly(self, update_stats):
        """åŸºäºæ›´æ–°èŒƒæ•°æ£€æµ‹å¼‚å¸¸"""
        total_norm = update_stats['total_norm']
        cv_norm = update_stats['cv_norm']
        max_norm = update_stats['max_norm']
        
        malicious_score = 0
        evidence = []
        
        # åŸºäºå®é™…æ•°æ®è°ƒæ•´é˜ˆå€¼ï¼šæ¶æ„å®¢æˆ·ç«¯èŒƒæ•°æ›´å°ï¼ˆæ ‡ç­¾ç¿»è½¬å¯¼è‡´æ›´æ–°æ··ä¹±ï¼‰
        # è‰¯æ€§ï¼šæ€»èŒƒæ•°~3200-3400ï¼Œæœ€å¤§å±‚èŒƒæ•°~740
        # æ¶æ„ï¼šæ€»èŒƒæ•°~1300-1400ï¼Œæœ€å¤§å±‚èŒƒæ•°~270-320
        if total_norm < 2000:  # åå‘ï¼šå°äº†æ‰æ˜¯æ¶æ„
            malicious_score += 1
            evidence.append(f"æ€»æ›´æ–°èŒƒæ•°è¿‡ä½: {total_norm:.2f} < 2000")
        
        if max_norm < 400:  # åå‘ï¼šå°äº†æ‰æ˜¯æ¶æ„
            malicious_score += 1
            evidence.append(f"æœ€å¤§å±‚èŒƒæ•°è¿‡ä½: {max_norm:.2f} < 400")
        
        is_anomaly = malicious_score >= 2
        confidence = malicious_score / 3.0
        
        return {
            'is_anomaly': is_anomaly,
            'confidence': confidence,
            'evidence': evidence,
            'malicious_score': malicious_score,
            'method': 'parameter_update_norm',
            'features': update_stats
        }


class UpdateDirectionDetector:
    """æ›´æ–°æ–¹å‘ä½™å¼¦ç›¸ä¼¼åº¦æ£€æµ‹å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = args.device
    
    def calculate_update_direction(self, global_model, external_model, tee_model):
        """è®¡ç®—æ›´æ–°æ–¹å‘ä½™å¼¦ç›¸ä¼¼åº¦"""
        global_params = dict(global_model.state_dict())
        external_params = dict(external_model.state_dict())
        tee_params = dict(tee_model.state_dict())
        
        # è®¡ç®—æ›´æ–°å‘é‡
        external_updates = []
        tee_updates = []
        
        for layer_name in global_params.keys():
            if layer_name in external_params and layer_name in tee_params:
                Î”_external = (external_params[layer_name] - global_params[layer_name]).flatten()
                Î”_tee = (tee_params[layer_name] - global_params[layer_name]).flatten()
                
                external_updates.append(Î”_external)
                tee_updates.append(Î”_tee)
        
        # å±•å¹³æˆä¸€ç»´å‘é‡
        external_update_flat = torch.cat(external_updates).cpu().numpy()
        tee_update_flat = torch.cat(tee_updates).cpu().numpy()
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(external_update_flat, tee_update_flat)
        norm_external = np.linalg.norm(external_update_flat)
        norm_tee = np.linalg.norm(tee_update_flat)
        
        if norm_external == 0 or norm_tee == 0:
            cosine_sim = 0.0
        else:
            cosine_sim = dot_product / (norm_external * norm_tee)
        
        # è®¡ç®—èŒƒæ•°æ¯”ä¾‹
        norm_ratio = norm_external / norm_tee if norm_tee > 0 else 0
        
        return {
            'update_direction_similarity': cosine_sim,
            'update_norm_ratio': norm_ratio,
            'external_norm': norm_external,
            'tee_norm': norm_tee
        }
    
    def detect_anomaly(self, direction_stats, threshold=0.1):
        """åŸºäºæ›´æ–°æ–¹å‘æ£€æµ‹å¼‚å¸¸ï¼ˆä»…ä½¿ç”¨direction_similarityï¼‰
        
        Args:
            direction_stats: æ–¹å‘ç»Ÿè®¡ä¿¡æ¯
            threshold: æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤0.1ï¼Œnoise_injectionæ—¶ä¸º0.2ï¼‰
        """
        cosine_sim = direction_stats['update_direction_similarity']
        norm_ratio = direction_stats['update_norm_ratio']
        
        # æ–¹å‘åç¦»æ£€æµ‹
        # é˜ˆå€¼æ ¹æ®æ”»å‡»ç±»å‹è°ƒæ•´ï¼š
        # - Label Flipping: 0.1ï¼ˆé›¶æ¼æ£€ç­–ç•¥ï¼‰
        # - Noise Injection: 0.2ï¼ˆå®½æ¾ç­–ç•¥ï¼Œé…åˆæ¬§æ°è·ç¦»ï¼‰
        if cosine_sim < threshold:
            is_anomaly = True
            evidence = [f"æ›´æ–°æ–¹å‘ä¸¥é‡åç¦»: cos={cosine_sim:.4f} < {threshold}"]
        else:
            is_anomaly = False
            evidence = [f"æ›´æ–°æ–¹å‘æ­£å¸¸: cos={cosine_sim:.4f} >= {threshold}"]
        
        # èŒƒæ•°æ¯”ç‡ä»…ä½œä¸ºå‚è€ƒï¼ˆä¸å½±å“åˆ¤æ–­ï¼‰
        evidence.append(f"èŒƒæ•°æ¯”ç‡(å‚è€ƒ): {norm_ratio:.4f}")
        
        return {
            'is_anomaly': is_anomaly,
            'confidence': 1.0 if is_anomaly else 0.0,
            'evidence': evidence,
            'malicious_score': 1 if is_anomaly else 0,
            'method': 'update_direction',
            'features': direction_stats
        }


class BatchNormEuclideanDetector:
    """BatchNormå±‚æ¬§æ°è·ç¦»æ£€æµ‹å™¨ï¼ˆå™ªå£°æ”»å‡»ä¸“ç”¨ï¼‰"""
    
    def __init__(self, args):
        self.args = args
        self.device = args.device
    
    def calculate_bn_euclidean(self, external_model, tee_model):
        """è®¡ç®—æ‰€æœ‰BatchNormå±‚çš„æ¬§æ°è·ç¦»ï¼ˆæ·±å±‚æœ€æ•æ„Ÿï¼šL2-L3-L4ï¼‰"""
        external_params = dict(external_model.state_dict())
        tee_params = dict(tee_model.state_dict())
        
        bn_distances = {}
        bn_layers = []
        sensitive_layer_distances = []  # æœ€æ•æ„Ÿå±‚ï¼ˆlayer2, layer3, layer4ï¼‰- æ·±å±‚BN
        shallow_layer_distances = []  # æµ…å±‚ï¼ˆlayer1, bn1ç­‰ï¼‰
        
        # éå†æ‰€æœ‰å±‚ï¼Œæ‰¾å‡ºBatchNormå±‚
        for layer_name in external_params.keys():
            # è¯†åˆ«BatchNormå±‚ï¼ˆåŒ…æ‹¬weight, bias, running_mean, running_varï¼‰
            # æˆ‘ä»¬ä¸»è¦å…³æ³¨å¯è®­ç»ƒå‚æ•°ï¼šweightå’Œbias
            if ('bn' in layer_name or 'norm' in layer_name.lower()) and \
               ('weight' in layer_name or 'bias' in layer_name):
                
                if layer_name in tee_params:
                    external_param = external_params[layer_name]
                    tee_param = tee_params[layer_name]
                    
                    # è®¡ç®—æ¬§æ°è·ç¦»
                    distance = torch.norm(external_param - tee_param).item()
                    bn_distances[layer_name] = distance
                    bn_layers.append(layer_name)
                    
                    # åˆ†å±‚ç»Ÿè®¡ï¼šåŸºäºNOISE_STD_010_ANALYSIS.mdæ–‡æ¡£
                    # TOP 5æœ€æ•æ„Ÿå±‚ï¼šlayer2 (d=1.761), layer3 (d=1.739), layer4 (d=1.672)
                    # æœ€æ•æ„Ÿå±‚ï¼šlayer2, layer3, layer4ï¼ˆæ·±å±‚BNï¼Œå™ªå£°é€å±‚ç´¯ç§¯ï¼‰
                    if 'layer2' in layer_name or 'layer3' in layer_name or 'layer4' in layer_name:
                        sensitive_layer_distances.append(distance)
                    else:  # layer1, bn1ç­‰æµ…å±‚
                        shallow_layer_distances.append(distance)
        
        if not bn_distances:
            return None
        
        # ç»Ÿè®¡ä¿¡æ¯
        distances_list = list(bn_distances.values())
        mean_distance = np.mean(distances_list)
        max_distance = np.max(distances_list)
        std_distance = np.std(distances_list)
        
        # æ•æ„Ÿå±‚å¹³å‡è·ç¦»ï¼ˆä¼˜å…ˆæŒ‡æ ‡ï¼‰
        sensitive_mean = np.mean(sensitive_layer_distances) if sensitive_layer_distances else 0.0
        shallow_mean = np.mean(shallow_layer_distances) if shallow_layer_distances else 0.0
        
        return {
            'bn_distances': bn_distances,
            'bn_layers': bn_layers,
            'mean_distance': mean_distance,
            'max_distance': max_distance,
            'std_distance': std_distance,
            'n_bn_layers': len(bn_distances),
            'sensitive_mean': sensitive_mean,  # æœ€æ•æ„Ÿå±‚å¹³å‡è·ç¦»ï¼ˆlayer2, layer3, layer4ï¼‰
            'shallow_mean': shallow_mean,  # æµ…å±‚å¹³å‡è·ç¦»ï¼ˆlayer1, bn1ç­‰ï¼‰
            'n_sensitive_layers': len(sensitive_layer_distances),
            'n_shallow_layers': len(shallow_layer_distances)
        }
    
    def detect_anomaly(self, bn_stats, threshold=0.008, use_sensitive_layers=True):
        """åŸºäºBatchNormå±‚æ¬§æ°è·ç¦»æ£€æµ‹å™ªå£°æ”»å‡»
        
        Args:
            bn_stats: BatchNormç»Ÿè®¡ä¿¡æ¯
            threshold: è·ç¦»é˜ˆå€¼ï¼ˆé»˜è®¤0.008ï¼ŒåŸºäºå®é™…è¿è¡Œæ•°æ®ï¼‰
            use_sensitive_layers: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ€æ•æ„Ÿå±‚BNï¼ˆlayer2, layer3, layer4ï¼‰
        
        åŸç†:
            å™ªå£°æ³¨å…¥ä¼šå½±å“æ·±å±‚BatchNormå±‚çš„å‚æ•°æ›´æ–°
            - æ·±å±‚BNå¹³å‡è·ç¦»ï¼ˆåŸºäºstd=0.2, Non-IIDå®é™…æ•°æ®ï¼‰
              * è‰¯æ€§å®¢æˆ·ç«¯å‡å€¼: 0.0061, ä¸­ä½æ•°: 0.0028
              * æ¶æ„å®¢æˆ·ç«¯å‡å€¼: 0.0104, ä¸­ä½æ•°: 0.0108
              * é˜ˆå€¼0.008: å¹³è¡¡è¯¯æŠ¥ç‡(~20%)å’Œæ¼æŠ¥ç‡(~15%)
            - åŸå› ï¼šæ·±å±‚ç‰¹å¾å—å™ªå£°å½±å“æ›´å¤§ï¼Œå™ªå£°é€å±‚ä¼ æ’­ç´¯ç§¯
            - è‰¯æ€§å®¢æˆ·ç«¯ï¼šBNå±‚å‚æ•°ä¸TEEç›¸è¿‘ï¼ˆè·ç¦»å°ï¼‰
            - æ¶æ„å®¢æˆ·ç«¯ï¼ˆå™ªå£°ï¼‰ï¼šBNå±‚å‚æ•°åç¦»TEEï¼ˆè·ç¦»å¤§ï¼‰
        """
        if bn_stats is None:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'evidence': ['æœªæ‰¾åˆ°BatchNormå±‚'],
                'method': 'batchnorm_euclidean'
            }
        
        # ä¼˜å…ˆä½¿ç”¨æœ€æ•æ„Ÿå±‚BNè·ç¦»ï¼ˆlayer2, layer3, layer4ï¼‰
        if use_sensitive_layers and bn_stats.get('n_sensitive_layers', 0) > 0:
            target_distance = bn_stats['sensitive_mean']
            distance_type = "æ·±å±‚BN(L2-L3-L4)"
            n_layers = bn_stats['n_sensitive_layers']
        else:
            target_distance = bn_stats['mean_distance']
            distance_type = "å…¨éƒ¨BN"
            n_layers = bn_stats['n_bn_layers']
        
        max_distance = bn_stats['max_distance']
        
        evidence = []
        
        # åŸºäºå®é™…è¿è¡Œæ•°æ®ï¼ˆstd=0.2, Non-IIDï¼‰ï¼š
        # - è‰¯æ€§å®¢æˆ·ç«¯ï¼š0.0016-0.0173ï¼ˆå‡å€¼0.0061ï¼Œä¸­ä½æ•°0.0028ï¼‰
        # - æ¶æ„å®¢æˆ·ç«¯ï¼š0.0015-0.0195ï¼ˆå‡å€¼0.0104ï¼Œä¸­ä½æ•°0.0108ï¼‰
        # - é˜ˆå€¼0.008ï¼šåœ¨å‡å€¼ä¹‹é—´ï¼Œå¹³è¡¡è¯¯æŠ¥å’Œæ¼æŠ¥
        # - é¢„æœŸï¼šå¬å›ç‡85%ï¼Œè¯¯æŠ¥ç‡20%
        if target_distance > threshold:
            is_anomaly = True
            evidence.append(f"{distance_type}è·ç¦»å¼‚å¸¸: {target_distance:.4f} > {threshold}")
        else:
            is_anomaly = False
            evidence.append(f"{distance_type}è·ç¦»æ­£å¸¸: {target_distance:.4f} <= {threshold}")
        
        evidence.append(f"æ£€æµ‹{n_layers}ä¸ª{distance_type}å±‚")
        evidence.append(f"æœ€å¤§è·ç¦»: {max_distance:.4f}")
        
        # æ·»åŠ æ•æ„Ÿå±‚/æµ…å±‚å¯¹æ¯”ä¿¡æ¯
        if 'sensitive_mean' in bn_stats and 'shallow_mean' in bn_stats:
            evidence.append(f"æ·±å±‚(L2-L3-L4): {bn_stats['sensitive_mean']:.4f}, æµ…å±‚(L1): {bn_stats['shallow_mean']:.4f}")
        
        return {
            'is_anomaly': is_anomaly,
            'confidence': 1.0 if is_anomaly else 0.0,
            'evidence': evidence,
            'malicious_score': 1 if is_anomaly else 0,
            'method': 'batchnorm_euclidean',
            'features': bn_stats
        }


class LayerWiseDirectionDetector:
    """
    å±‚çº§æ–¹å‘æ£€æµ‹å™¨ - åœ¨Non-IIDç¯å¢ƒä¸‹ä¾ç„¶èƒ½å¤Ÿå‡†ç¡®æ£€æµ‹æ¶æ„å®¢æˆ·ç«¯
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - Non-IIDç¯å¢ƒä¸‹ï¼Œå®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒä¸åŒï¼Œå¯¼è‡´ä¼ ç»Ÿæ£€æµ‹å™¨å¤±æ•ˆ
    - ä½¿ç”¨TEEå†…çš„å…¨å±€IIDéªŒè¯é›†ä½œä¸º"æ ‡å‡†å‚ç…§"
    - è‰¯æ€§å®¢æˆ·ç«¯ï¼šæ— è®ºæ•°æ®å¦‚ä½•Non-IIDï¼Œæ›´æ–°æ–¹å‘åº”è¯¥æœå‘å…¨å±€æœ€ä¼˜
    - æ¶æ„å®¢æˆ·ç«¯ï¼šæ”»å‡»ä¼šå¯¼è‡´æ›´æ–°æ–¹å‘ä¸¥é‡åç¦»å…¨å±€æœ€ä¼˜
    """
    
    def __init__(self, args):
        self.args = args
        self.device = args.device
    
    def calculate_validation_gradient(self, model, validation_loader):
        """
        åœ¨éªŒè¯é›†ä¸Šè®¡ç®—æ¢¯åº¦ï¼ˆå…¨å±€æœ€ä¼˜æ–¹å‘ï¼‰
        éªŒè¯é›†æ˜¯å…¨å±€IIDçš„ï¼Œä»£è¡¨äº†æ¨¡å‹åº”è¯¥ä¼˜åŒ–çš„çœŸå®æ–¹å‘
        """
        model.train()
        model.zero_grad()
        
        criterion = nn.CrossEntropyLoss()
        total_loss = 0
        batch_count = 0
        
        # åœ¨éªŒè¯é›†ä¸Šè®¡ç®—æ¢¯åº¦
        for images, labels in validation_loader:
            images, labels = images.to(self.device), labels.to(self.device)
            
            outputs = model(images)
            
            # å¤„ç†æ¨¡å‹å¯èƒ½è¿”å›å­—å…¸çš„æƒ…å†µ
            if isinstance(outputs, dict):
                outputs = outputs.get('logits', outputs.get('output', list(outputs.values())[0]))
            
            loss = criterion(outputs, labels)
            loss.backward()
            
            total_loss += loss.item()
            batch_count += 1
            
            # åªä½¿ç”¨å‰å‡ ä¸ªbatchä»¥èŠ‚çœè®¡ç®—
            if batch_count >= 3:
                break
        
        # æå–æ¢¯åº¦
        validation_gradients = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                validation_gradients[name] = param.grad.clone().detach()
        
        model.zero_grad()
        
        return validation_gradients, total_loss / batch_count
    
    def calculate_layer_direction_similarity(self, global_model, external_model, tee_model, validation_loader):
        """
        è®¡ç®—å„å±‚çš„æ–¹å‘ç›¸ä¼¼åº¦
        
        åŸç†ï¼š
        1. åœ¨å…¨å±€IIDéªŒè¯é›†ä¸Šè®¡ç®—æ¢¯åº¦ï¼ˆå…¨å±€æœ€ä¼˜æ–¹å‘ï¼‰
        2. è®¡ç®—å¤–éƒ¨æ¨¡å‹çš„æ›´æ–°æ–¹å‘
        3. æ¯”è¾ƒå¤–éƒ¨æ¨¡å‹æ–¹å‘ä¸å…¨å±€æœ€ä¼˜æ–¹å‘çš„ç›¸ä¼¼åº¦
        4. åœ¨Non-IIDä¸‹ï¼š
           - è‰¯æ€§å®¢æˆ·ç«¯ï¼šè™½ç„¶æ•°æ®Non-IIDï¼Œä½†æ–¹å‘åº”æ¥è¿‘å…¨å±€æœ€ä¼˜
           - æ¶æ„å®¢æˆ·ç«¯ï¼šæ”»å‡»å¯¼è‡´æ–¹å‘ä¸¥é‡åç¦»
        """
        # è®¡ç®—æ¨¡å‹æ›´æ–°
        global_params = dict(global_model.state_dict())
        external_params = dict(external_model.state_dict())
        tee_params = dict(tee_model.state_dict())
        
        # è®¡ç®—å¤–éƒ¨æ¨¡å‹å’ŒTEEæ¨¡å‹çš„å‚æ•°æ›´æ–°
        external_updates = {}
        tee_updates = {}
        
        for name in global_params.keys():
            if name in external_params and name in tee_params:
                external_updates[name] = external_params[name] - global_params[name]
                tee_updates[name] = tee_params[name] - global_params[name]
        
        # åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å…¨å±€æœ€ä¼˜æ–¹å‘ï¼ˆä½¿ç”¨å…¨å±€æ¨¡å‹ï¼‰
        validation_model = copy.deepcopy(global_model)
        validation_gradients, val_loss = self.calculate_validation_gradient(validation_model, validation_loader)
        
        # è®¡ç®—å„å±‚çš„æ–¹å‘ç›¸ä¼¼åº¦
        layer_similarities = {}
        important_layers = []
        
        for name in external_updates.keys():
            # åªå…³æ³¨æƒé‡å±‚
            if 'weight' in name and name in validation_gradients:
                # ä¼°ç®—æ¢¯åº¦ï¼ˆSGD: update = -lr * gradï¼‰
                external_grad = -external_updates[name] / self.args.lr
                tee_grad = -tee_updates[name] / self.args.lr
                val_grad = validation_gradients[name]
                
                # å±•å¹³
                external_grad_flat = external_grad.flatten()
                tee_grad_flat = tee_grad.flatten()
                val_grad_flat = val_grad.flatten()
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                external_val_sim = torch.nn.functional.cosine_similarity(
                    external_grad_flat.unsqueeze(0), val_grad_flat.unsqueeze(0)
                ).item()
                
                tee_val_sim = torch.nn.functional.cosine_similarity(
                    tee_grad_flat.unsqueeze(0), val_grad_flat.unsqueeze(0)
                ).item()
                
                layer_similarities[name] = {
                    'external_val_similarity': external_val_sim,
                    'tee_val_similarity': tee_val_sim,
                    'direction_deviation': abs(external_val_sim - tee_val_sim)
                }
                important_layers.append(name)
        
        if not layer_similarities:
            return None
        
        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        external_sims = [v['external_val_similarity'] for v in layer_similarities.values()]
        tee_sims = [v['tee_val_similarity'] for v in layer_similarities.values()]
        deviations = [v['direction_deviation'] for v in layer_similarities.values()]
        
        return {
            'layer_similarities': layer_similarities,
            'mean_external_similarity': np.mean(external_sims),
            'mean_tee_similarity': np.mean(tee_sims),
            'mean_deviation': np.mean(deviations),
            'max_deviation': np.max(deviations),
            'validation_loss': val_loss,
            'n_layers': len(layer_similarities)
        }
    
    def detect_anomaly(self, direction_stats):
        """åŸºäºå±‚çº§æ–¹å‘æ£€æµ‹å¼‚å¸¸"""
        if direction_stats is None:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'evidence': ['æ— æœ‰æ•ˆå±‚æ•°æ®'],
                'method': 'layer_wise_direction',
                'features': {}
            }
        
        mean_external_sim = direction_stats['mean_external_similarity']
        mean_tee_sim = direction_stats['mean_tee_similarity']
        mean_deviation = direction_stats['mean_deviation']
        max_deviation = direction_stats['max_deviation']
        
        malicious_score = 0
        evidence = []
        
        # æ£€æµ‹å¤–éƒ¨æ¨¡å‹ä¸å…¨å±€æœ€ä¼˜æ–¹å‘çš„åç¦»
        # åœ¨Non-IIDä¸‹ï¼Œè‰¯æ€§å®¢æˆ·ç«¯çš„æ›´æ–°æ–¹å‘åº”è¯¥ä»ç„¶æœå‘å…¨å±€æœ€ä¼˜
        if mean_external_sim < 0.3:  # å¤–éƒ¨æ¨¡å‹æ–¹å‘ä¸å…¨å±€æœ€ä¼˜æ–¹å‘ç›¸å
            malicious_score += 1
            evidence.append(f"æ›´æ–°æ–¹å‘ä¸¥é‡åç¦»å…¨å±€æœ€ä¼˜: {mean_external_sim:.4f} < 0.3")
        
        # æ£€æµ‹å¤–éƒ¨æ¨¡å‹ä¸TEEæ¨¡å‹çš„æ–¹å‘å·®å¼‚
        # TEEæ¨¡å‹åœ¨ç›¸åŒæ•°æ®ä¸Šè®­ç»ƒï¼Œæ–¹å‘åº”è¯¥ä¸€è‡´
        if mean_deviation > 0.5:  # æ–¹å‘å·®å¼‚è¿‡å¤§
            malicious_score += 1
            evidence.append(f"å¤–éƒ¨ä¸TEEæ–¹å‘å·®å¼‚è¿‡å¤§: {mean_deviation:.4f} > 0.5")
        
        # æ£€æµ‹æœ€å¤§å±‚åç¦»ï¼ˆæŸäº›å±‚å¯èƒ½è¢«ç‰¹åˆ«æ”»å‡»ï¼‰
        if max_deviation > 0.7:
            malicious_score += 1
            evidence.append(f"å­˜åœ¨ä¸¥é‡åç¦»å±‚: {max_deviation:.4f} > 0.7")
        
        is_anomaly = malicious_score >= 2
        confidence = malicious_score / 3.0
        
        return {
            'is_anomaly': is_anomaly,
            'confidence': confidence,
            'malicious_score': malicious_score,
            'evidence': evidence if evidence else ['æ–¹å‘æ­£å¸¸'],
            'method': 'layer_wise_direction',
            'features': {
                'mean_external_similarity': mean_external_sim,
                'mean_tee_similarity': mean_tee_sim,
                'mean_deviation': mean_deviation,
                'max_deviation': max_deviation
            }
        }


class LayerWiseUpdateNormDetector:
    """å„å±‚æ›´æ–°èŒƒæ•°æ£€æµ‹å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = args.device
    
    def calculate_layer_wise_norms(self, global_model, external_model):
        """è®¡ç®—å„å±‚çš„æ›´æ–°èŒƒæ•°"""
        global_params = dict(global_model.state_dict())
        external_params = dict(external_model.state_dict())
        
        layer_norms = {}
        important_layers = []
        
        for layer_name in global_params.keys():
            # åªå…³æ³¨é‡è¦å±‚ï¼ˆå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚çš„æƒé‡ï¼‰
            if 'weight' in layer_name and ('conv' in layer_name or 'linear' in layer_name or 'layer' in layer_name):
                if layer_name in external_params:
                    param_update = external_params[layer_name] - global_params[layer_name]
                    estimated_gradient = -param_update / self.args.lr
                    layer_norm = torch.norm(estimated_gradient).item()
                    layer_norms[layer_name] = layer_norm
                    important_layers.append(layer_name)
        
        if not layer_norms:
            return None
        
        # åˆ†æå„å±‚èŒƒæ•°çš„åˆ†å¸ƒ
        norms_list = list(layer_norms.values())
        mean_norm = np.mean(norms_list)
        std_norm = np.std(norms_list)
        
        # è®¡ç®—é«˜èŒƒæ•°å±‚çš„æ¯”ä¾‹
        threshold = mean_norm + 1.0 * std_norm
        high_norm_count = sum(1 for n in norms_list if n > threshold)
        high_norm_ratio = high_norm_count / len(norms_list)
        
        return {
            'layer_norms': layer_norms,
            'mean_norm': mean_norm,
            'std_norm': std_norm,
            'high_norm_ratio': high_norm_ratio,
            'n_layers': len(layer_norms)
        }
    
    def detect_anomaly(self, layer_stats):
        """åŸºäºå„å±‚èŒƒæ•°ä¸€è‡´æ€§æ£€æµ‹å¼‚å¸¸"""
        if layer_stats is None:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'evidence': ['æ— æœ‰æ•ˆå±‚æ•°æ®'],
                'method': 'layer_wise_update_norm'
            }
        
        high_norm_ratio = layer_stats['high_norm_ratio']
        mean_norm = layer_stats['mean_norm']
        
        malicious_score = 0
        evidence = []
        
        # è¶…è¿‡90%çš„å±‚èŒƒæ•°éƒ½å¼‚å¸¸é«˜ï¼ˆæ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼‰
        if high_norm_ratio > 0.9:
            malicious_score += 1
            evidence.append(f"é«˜èŒƒæ•°å±‚æ¯”ä¾‹: {high_norm_ratio:.2%} > 90%")
        
        # åŸºäºå®é™…æ•°æ®ï¼šæ¶æ„å®¢æˆ·ç«¯èŒƒæ•°æ›´å°
        # è‰¯æ€§ï¼šå¹³å‡å±‚èŒƒæ•°~340ï¼Œæ¶æ„ï¼šå¹³å‡å±‚èŒƒæ•°~145
        if mean_norm < 200:  # åå‘ï¼šå°äº†æ‰æ˜¯æ¶æ„
            malicious_score += 1
            evidence.append(f"å¹³å‡å±‚èŒƒæ•°è¿‡ä½: {mean_norm:.2f} < 200")
        
        is_anomaly = malicious_score >= 1
        confidence = malicious_score / 2.0
        
        return {
            'is_anomaly': is_anomaly,
            'confidence': confidence,
            'evidence': evidence,
            'malicious_score': malicious_score,
            'method': 'layer_wise_update_norm',
            'features': layer_stats
        }


# ===================== ç‹¬ç«‹æ£€æµ‹å™¨æµ‹è¯•ç±» =====================

class IndependentDetectorsTester:
    """ç‹¬ç«‹æ£€æµ‹å™¨æµ‹è¯•å™¨ - ä¸èšåˆï¼Œå„è‡ªæ£€æµ‹"""
    
    def __init__(self, args):
        self.args = args
        self.device = args.device
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨ï¼ˆä»…ä½¿ç”¨å®é™…éœ€è¦çš„æ£€æµ‹å™¨ï¼‰
        self.detectors = {
            'parameter_update_norm': ParameterUpdateNormDetector(args),
            'update_direction': UpdateDirectionDetector(args),
            'batchnorm_euclidean': BatchNormEuclideanDetector(args),  # å™ªå£°æ”»å‡»ä¸“ç”¨ï¼ˆå·²å±è”½ä½†ä¿ç•™å®šä¹‰ï¼‰
            'layer_wise_update_norm': LayerWiseUpdateNormDetector(args),
            'layer_wise_direction': LayerWiseDirectionDetector(args),
        }
        
        # æ£€æµ‹å™¨æ˜¾ç¤ºåç§°
        self.detector_names = {
            'parameter_update_norm': 'å‚æ•°æ›´æ–°èŒƒæ•°',
            'update_direction': 'æ›´æ–°æ–¹å‘',
            'batchnorm_euclidean': 'BNå±‚æ¬§æ°è·ç¦»ï¼ˆå™ªå£°æ”»å‡»ä¸“ç”¨ï¼‰',
            'layer_wise_update_norm': 'å±‚çº§æ›´æ–°èŒƒæ•°',
            'layer_wise_direction': 'å±‚çº§æ–¹å‘ï¼ˆNon-IIDé€‚ç”¨ï¼‰',
        }
        
        # ç»“æœè®°å½•
        self.results = {}
    
    def test_update_direction_only(self, global_model, external_model, tee_model, client_id, 
                                   is_malicious, validation_loader=None, attack_scenario='label_flipping'):
        """
        ä»…æµ‹è¯•update_directionæ£€æµ‹å™¨ï¼ˆç”¨äºå®é™…éƒ¨ç½²ï¼‰
        noise_injectionæ—¶ä¼šé¢å¤–å¯ç”¨BatchNormæ¬§æ°è·ç¦»æ£€æµ‹å™¨
        
        Args:
            global_model: å…¨å±€æ¨¡å‹
            external_model: å¤–éƒ¨è®­ç»ƒçš„æ¨¡å‹
            tee_model: TEEè®­ç»ƒçš„æ¨¡å‹
            client_id: å®¢æˆ·ç«¯ID
            is_malicious: çœŸå®æ ‡ç­¾ï¼ˆæ˜¯å¦æ¶æ„ï¼‰
            validation_loader: TEEéªŒè¯é›†ï¼ˆæœ¬æ£€æµ‹å™¨ä¸éœ€è¦ï¼‰
            attack_scenario: æ”»å‡»åœºæ™¯ ('label_flipping' æˆ– 'noise_injection')
        
        Returns:
            dict: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸
        """
        print(f"  [æ£€æµ‹] å®¢æˆ·ç«¯ {client_id} ({'æ¶æ„' if is_malicious else 'è‰¯æ€§'})")
        
        client_results = {
            'client_id': client_id,
            'is_malicious': is_malicious,
            'attack_scenario': attack_scenario,
            'detectors': {}
        }
        
        # æ ¹æ®æ”»å‡»ç±»å‹è®¾ç½®é˜ˆå€¼
        is_noise_attack = (attack_scenario == 'noise_injection')
        direction_threshold = 0.24 if is_noise_attack else 0.1  # std=0.25ä¼˜åŒ–é˜ˆå€¼
        
        # 1. æ›´æ–°æ–¹å‘æ£€æµ‹ï¼ˆå¿…é€‰ï¼‰
        try:
            # è®¡ç®—æ›´æ–°æ–¹å‘æŒ‡æ ‡
            direction_stats = self.detectors['update_direction'].calculate_update_direction(
                global_model, external_model, tee_model
            )
            
            # åŸºäºæŒ‡æ ‡è¿›è¡Œæ£€æµ‹ï¼ˆä¼ å…¥é˜ˆå€¼ï¼‰
            update_direction_result = self.detectors['update_direction'].detect_anomaly(
                direction_stats, threshold=direction_threshold
            )
            
            # ä¿å­˜ç»“æœ
            client_results['detectors']['update_direction'] = {
                'detection_result': update_direction_result,
                'metadata': {
                    'method': 'update_direction',
                    'requires_tee': True,
                    'threshold': direction_threshold
                }
            }
            
        except Exception as e:
            print(f"    âŒ æ›´æ–°æ–¹å‘æ£€æµ‹é”™è¯¯: {str(e)}")
            import traceback
            client_results['detectors']['update_direction'] = {
                'error': str(e),
                'traceback': traceback.format_exc()
            }
        
        # 2. BNå±‚æ¬§æ°è·ç¦»æ£€æµ‹å·²å±è”½ï¼ˆä»…ä½¿ç”¨direction_similarityæ£€æµ‹ï¼‰
        # æ³¨é‡Šæ‰BNæ£€æµ‹å™¨ï¼Œç»Ÿä¸€ä½¿ç”¨direction_similarityæ£€æµ‹
        # if is_noise_attack:
        #     try:
        #         # è®¡ç®—BNå±‚æ¬§æ°è·ç¦»
        #         bn_stats = self.detectors['batchnorm_euclidean'].calculate_bn_euclidean(
        #             external_model, tee_model
        #         )
        #         
        #         # åŸºäºæŒ‡æ ‡è¿›è¡Œæ£€æµ‹ï¼ˆä½¿ç”¨æ·±å±‚BNï¼šlayer2, layer3, layer4ï¼Œé˜ˆå€¼0.008ï¼‰
        #         bn_euclidean_result = self.detectors['batchnorm_euclidean'].detect_anomaly(
        #             bn_stats, threshold=0.008, use_sensitive_layers=True
        #         )
        #         
        #         # ä¿å­˜ç»“æœ
        #         client_results['detectors']['batchnorm_euclidean'] = {
        #             'detection_result': bn_euclidean_result,
        #             'metadata': {
        #                 'method': 'batchnorm_euclidean',
        #                 'requires_tee': True,
        #                 'threshold': 0.15
        #             }
        #         }
        #         
        #     except Exception as e:
        #         print(f"    âŒ BNå±‚æ¬§æ°è·ç¦»æ£€æµ‹é”™è¯¯: {str(e)}")
        #         import traceback
        #         client_results['detectors']['batchnorm_euclidean'] = {
        #             'error': str(e),
        #             'traceback': traceback.format_exc()
        #         }
        
        return client_results
    
    def _print_result(self, detector_name, result, actual_malicious):
        """æ‰“å°æ£€æµ‹ç»“æœ"""
        display_name = self.detector_names.get(detector_name, detector_name)
        
        if 'error' in result:
            print(f"   âŒ {display_name}: é”™è¯¯")
            return
        
        is_anomaly = result.get('is_anomaly', False)
        confidence = result.get('confidence', 0.0)
        
        # åˆ¤æ–­æ£€æµ‹æ˜¯å¦æ­£ç¡®
        correct = (is_anomaly == actual_malicious)
        symbol = "âœ…" if correct else "âŒ"
        
        result_text = "æ¶æ„" if is_anomaly else "è‰¯æ€§"
        actual_text = "æ¶æ„" if actual_malicious else "è‰¯æ€§"
        
        print(f"   {symbol} {display_name}: æ£€æµ‹={result_text}, å®é™…={actual_text}, ç½®ä¿¡åº¦={confidence:.3f}")
        
        # æ‰“å°è¯¦ç»†æŒ‡æ ‡
        if 'features' in result:
            features = result['features']
            print(f"      ğŸ“Š æ£€æµ‹ç‰¹å¾:")
            
            # æ ¹æ®ä¸åŒæ£€æµ‹å™¨æ˜¾ç¤ºä¸åŒçš„ç‰¹å¾
            if detector_name == 'parameter_update_norm':
                print(f"         æ€»èŒƒæ•°: {features.get('total_norm', 0):.2f}")
                print(f"         æœ€å¤§å±‚èŒƒæ•°: {features.get('max_norm', 0):.2f}")
                print(f"         å˜å¼‚ç³»æ•°: {features.get('cv_norm', 0):.4f}")
            
            elif detector_name == 'update_direction':
                print(f"         ä½™å¼¦ç›¸ä¼¼åº¦: {features.get('update_direction_similarity', 0):.4f}")
                print(f"         èŒƒæ•°æ¯”ä¾‹: {features.get('update_norm_ratio', 0):.4f}")
            
            elif detector_name == 'layer_wise_update_norm':
                print(f"         å¹³å‡å±‚èŒƒæ•°: {features.get('mean_norm', 0):.2f}")
                print(f"         é«˜èŒƒæ•°å±‚æ¯”ä¾‹: {features.get('high_norm_ratio', 0):.2%}")
            
            elif detector_name == 'layer_wise_direction':
                print(f"         å¤–éƒ¨æ¨¡å‹ç›¸ä¼¼åº¦: {features.get('mean_external_similarity', 0):.4f}")
                print(f"         æ–¹å‘åç¦»ç¨‹åº¦: {features.get('mean_deviation', 0):.4f}")
                print(f"         æœ€å¤§åç¦»: {features.get('max_deviation', 0):.4f}")
        
        # æ‰“å°è¯æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'evidence' in result and result['evidence']:
            print(f"      âš ï¸  å¼‚å¸¸è¯æ®:")
            for evidence in result['evidence'][:2]:  # åªæ˜¾ç¤ºå‰2æ¡
                print(f"         â€¢ {evidence}")
    
    def calculate_simple_statistics(self, all_results):
        """
        è®¡ç®—ç®€åŒ–ç»Ÿè®¡æŒ‡æ ‡ï¼ˆä»…update_directionæ£€æµ‹å™¨ï¼‰
        
        Args:
            all_results: æ‰€æœ‰å®¢æˆ·ç«¯çš„æ£€æµ‹ç»“æœåˆ—è¡¨
        
        Returns:
            dict: ç»Ÿè®¡ç»“æœ
        """
        print(f"\n{'='*80}")
        print("æ£€æµ‹å™¨æ€§èƒ½ç»Ÿè®¡ï¼ˆupdate_directionï¼‰")
        print(f"{'='*80}\n")
        
        tp = fp = tn = fn = 0
        
        for client_result in all_results:
            actual_malicious = client_result['is_malicious']
            detector_result = client_result['detectors'].get('update_direction', {})
            
            if 'error' in detector_result:
                continue
            
            # è·å–æ£€æµ‹ç»“æœ
            if 'detection_result' in detector_result:
                detection_info = detector_result['detection_result']
            else:
                detection_info = detector_result
            
            if 'is_anomaly' not in detection_info:
                continue
            
            detected_malicious = detection_info['is_anomaly']
            
            if actual_malicious and detected_malicious:
                tp += 1
            elif not actual_malicious and detected_malicious:
                fp += 1
            elif not actual_malicious and not detected_malicious:
                tn += 1
            elif actual_malicious and not detected_malicious:
                fn += 1
        
        total = tp + fp + tn + fn
        if total == 0:
            print("âš ï¸  æ— æœ‰æ•ˆæ£€æµ‹æ•°æ®")
            return {}
        
        accuracy = (tp + tn) / total if total > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
        
        detector_stats = {
            'update_direction': {
                'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'fpr': fpr,
                'fnr': fnr
            }
        }
        
        print("update_directionæ£€æµ‹å™¨:")
        print(f"  âœ… å‡†ç¡®ç‡: {accuracy:.2%}")
        print(f"  ğŸ“Š ç²¾ç¡®ç‡: {precision:.2%}  å¬å›ç‡: {recall:.2%}  F1åˆ†æ•°: {f1:.2%}")
        print(f"  âš ï¸  è¯¯æŠ¥ç‡: {fpr:.2%}  æ¼æŠ¥ç‡: {fnr:.2%}")
        print(f"  ğŸ“ˆ TP={tp}, FP={fp}, TN={tn}, FN={fn}")
        print(f"  ğŸ“ æ€»è®¡: {total} ä¸ªå®¢æˆ·ç«¯æ£€æµ‹")
        print()
        
        return detector_stats
    
    def _convert_to_json_serializable(self, obj):
        """é€’å½’è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, torch.device):
            return str(obj)
        elif isinstance(obj, dict):
            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        else:
            # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            try:
                json.dumps(obj)
                return obj
            except (TypeError, ValueError):
                return str(obj)
    
    def save_results(self, all_results, detector_stats, filename=None, attack_config=None, round_details=None):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            all_results: æ‰€æœ‰å®¢æˆ·ç«¯çš„æ£€æµ‹ç»“æœ
            detector_stats: æ£€æµ‹å™¨ç»Ÿè®¡ä¿¡æ¯
            filename: ä¿å­˜çš„æ–‡ä»¶å
            attack_config: æ”»å‡»é…ç½®ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            round_details: æ¯è½®è®­ç»ƒè¯¦æƒ…ï¼ˆå¯é€‰ï¼‰
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"independent_detectors_test_{timestamp}.json"
        
        # è½¬æ¢argsä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        args_dict = {}
        for key, value in vars(self.args).items():
            args_dict[key] = self._convert_to_json_serializable(value)
        
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæœ‰æ”»å‡»é…ç½®ï¼Œæ›´æ–°argsä¸­çš„attack_scenario
        if attack_config and 'attack_type' in attack_config:
            args_dict['attack_scenario'] = attack_config['attack_type']
        
        # è½¬æ¢æ‰€æœ‰æ•°æ®ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        output = {
            'test_time': datetime.now().isoformat(),
            'args': args_dict,
            'attack_config': self._convert_to_json_serializable(attack_config) if attack_config else None,
            'detector_stats': self._convert_to_json_serializable(detector_stats),
            'client_results': self._convert_to_json_serializable(all_results),
            'round_details': self._convert_to_json_serializable(round_details) if round_details else None
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename


def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    print("ç‹¬ç«‹æ£€æµ‹å™¨æµ‹è¯•ç³»ç»Ÿ")
    print("æµ‹è¯•update_directionæ£€æµ‹å™¨çš„ç‹¬ç«‹æ£€æµ‹æ•ˆæœ")
    
    # è¿™é‡Œåº”è¯¥å¯¼å…¥å®Œæ•´çš„è®­ç»ƒç¯å¢ƒ
    # å®é™…ä½¿ç”¨æ—¶éœ€è¦é›†æˆåˆ°è®­ç»ƒæµç¨‹ä¸­
    print("\nâš ï¸  è¯·åœ¨è®­ç»ƒæµç¨‹ä¸­è°ƒç”¨æ­¤æ¨¡å—")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  from independent_detectors_test import IndependentDetectorsTester")
    print("  tester = IndependentDetectorsTester(args)")
    print("  result = tester.test_update_direction_only(global_model, external_model, tee_model, client_id, is_malicious)")


if __name__ == "__main__":
    main()

